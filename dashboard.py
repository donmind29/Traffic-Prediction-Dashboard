import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
import joblib
import shap
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
from datetime import datetime, timedelta
import warnings

warnings.filterwarnings('ignore')

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="XGBoost Traffic Speed Prediction Dashboard",
    page_icon="ğŸš—",
    layout="wide",
    initial_sidebar_state="expanded"
)

# è‡ªå®šä¹‰CSSæ ·å¼
st.markdown("""
    <style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        color: #2E86AB;
        text-align: center;
        margin-bottom: 2rem;
        text-shadow: 2px 2px 4px rgba(0,0,0,0.1);
    }
    .metric-card {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 10px;
        box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    }
    .stTabs [data-baseweb="tab-list"] {
        gap: 24px;
    }
    .stTabs [data-baseweb="tab"] {
        height: 50px;
        padding-left: 20px;
        padding-right: 20px;
        background-color: #f0f2f6;
        border-radius: 10px 10px 0 0;
    }
    .stTabs [aria-selected="true"] {
        background-color: #2E86AB;
        color: white;
    }
    </style>
    """, unsafe_allow_html=True)


# ---- ADDED ---- helper to find street column consistently
def find_street_column(df, candidates=None):
    """è¿”å›æ•°æ®ä¸­ç¬¬ä¸€ä¸ªå­˜åœ¨çš„è¡—é“åˆ—åï¼ˆæˆ– Noneï¼‰ã€‚"""
    if df is None:
        return None
    if candidates is None:
        candidates = ['streetName', 'road_name', 'street', 'segment_id', 'link_id', 'road_id']
    for col in candidates:
        if col in df.columns:
            return col
    return None


# ç¼“å­˜å‡½æ•°ç”¨äºåŠ è½½æ¨¡å‹å’Œæ•°æ®
@st.cache_resource
def load_models():
    """åŠ è½½æ‰€æœ‰ä¿å­˜çš„æ¨¡å‹"""
    try:
        models_dict = joblib.load('all_models.pkl')
        preprocessing_info = joblib.load('preprocessing_info.pkl')
        results_df = joblib.load('hyperparameter_tuning_results.pkl')
        return models_dict, preprocessing_info, results_df
    except Exception as e:
        st.error(f"Model loading failed: {str(e)}")
        return None, None, None


@st.cache_data
def load_original_data():
    """åŠ è½½åŸå§‹æ•°æ®"""
    try:
        df = pd.read_csv('Johor_Bahru_hour_with_POI.csv')
        return df
    except Exception as e:
        st.error(f"Data loading failed: {str(e)}")
        return None


@st.cache_data
def get_top_streets(df, top_n=10):
    """è·å–æ ·æœ¬æ•°å‰Næ¡è¡—é“ï¼ˆä½¿ç”¨ find_street_column ç»Ÿä¸€è¯†åˆ«åˆ—åï¼‰"""
    if df is None:
        return []

    street_col = find_street_column(df)
    if street_col:
        street_counts = df[street_col].value_counts()
        return street_counts.head(top_n).index.tolist()
    else:
        # å¦‚æœæ²¡æœ‰è¡—é“åˆ—ï¼Œè¿”å›å‰Nä¸ªå”¯ä¸€æ ‡è¯†
        if 'index' in df.columns:
            return df['index'].unique()[:top_n].tolist()
        else:
            return list(range(top_n))


def create_future_prediction_interface(model, scaler, feature_names, original_df=None):
    """åˆ›å»ºæœªæ¥é¢„æµ‹ç•Œé¢ï¼ˆPrediction Settings ç§»åˆ°ä¾§è¾¹æ ï¼‰"""

    # åˆå§‹åŒ–session state
    if 'prediction_results' not in st.session_state:
        st.session_state.prediction_results = None
    if 'shap_results' not in st.session_state:
        st.session_state.shap_results = None
    if 'prediction_params' not in st.session_state:
        st.session_state.prediction_params = None

    # è·å–å‰10æ¡è¡—é“
    top_streets = get_top_streets(original_df, top_n=10)

    # === åœ¨ä¾§è¾¹æ æ·»åŠ é¢„æµ‹è®¾ç½® ===
    with st.sidebar:
        st.divider()
        st.markdown("### ğŸ”® Prediction Settings")

        # é€‰æ‹©è¡—é“
        selected_street = st.selectbox(
            "Select Street",
            options=top_streets,
            format_func=lambda x: f"Street {x}" if isinstance(x, (int, float)) else str(x),
            help="Select a street to predict (Top 10 by sample count)",
            key="sidebar_street_selector"
        )

        # é€‰æ‹©é¢„æµ‹æ—¶é—´èŒƒå›´
        st.markdown("#### Time Range")

        # å¼€å§‹æ—¥æœŸ
        start_date = st.date_input(
            "Start Date",
            value=datetime.now().date(),
            min_value=datetime.now().date(),
            max_value=datetime.now().date() + timedelta(days=30),
            key="sidebar_start_date"
        )

        # é¢„æµ‹å¤©æ•°
        prediction_days = st.slider(
            "Prediction Days",
            min_value=1,
            max_value=7,
            value=3,
            help="Select number of days to predict (1-7 days)",
            key="sidebar_prediction_days"
        )

        # æ—¶é—´ç²’åº¦
        time_granularity = st.radio(
            "Time Granularity",
            options=["Hourly", "Peak Hours", "Daily Average"],
            index=0,
            help="Select prediction time granularity",
            key="sidebar_time_granularity"
        )

        # é¢„æµ‹æŒ‰é’®
        predict_button = st.button("ğŸš€ Start Prediction", use_container_width=True, type="primary",
                                   key="sidebar_predict_btn")

    # === ä¸»ç•Œé¢åªæ˜¾ç¤ºé¢„æµ‹ç»“æœ ===
    # åªæœ‰ç‚¹å‡»é¢„æµ‹æŒ‰é’®æ—¶æ‰é‡æ–°è®¡ç®—
    if predict_button:
        with st.spinner("Generating predictions..."):
            try:
                # ç”Ÿæˆé¢„æµ‹æ—¶é—´åºåˆ—å¹¶åŒæ—¶è¿”å›ç‰¹å¾çŸ©é˜µï¼ˆåŸå§‹ã€ç¼©æ”¾ï¼‰
                predictions, feature_df, feature_df_scaled = generate_future_predictions(
                    model, scaler, feature_names, original_df,
                    selected_street, start_date, prediction_days, time_granularity
                )

                # ä¿å­˜é¢„æµ‹ç»“æœåˆ°session state
                st.session_state.prediction_results = {
                    'predictions': predictions,
                    'feature_df': feature_df,
                    'feature_df_scaled': feature_df_scaled
                }

                # ä¿å­˜é¢„æµ‹å‚æ•°
                st.session_state.prediction_params = {
                    'selected_street': selected_street,
                    'start_date': start_date,
                    'prediction_days': prediction_days,
                    'time_granularity': time_granularity
                }

                # è®¡ç®—SHAPå€¼å¹¶ä¿å­˜
                try:
                    explainer = shap.TreeExplainer(model)
                    shap_vals = explainer.shap_values(feature_df_scaled)
                    if isinstance(shap_vals, list) and len(shap_vals) == 1:
                        shap_vals = shap_vals[0]

                    st.session_state.shap_results = {
                        'shap_vals': shap_vals,
                        'explainer': explainer
                    }
                except Exception as e:
                    st.session_state.shap_results = None
                    st.warning(f"Unable to calculate SHAP values: {e}")

            except Exception as e:
                st.error(f"Prediction failed: {str(e)}")
                st.session_state.prediction_results = None
                st.session_state.shap_results = None

    # æ˜¾ç¤ºé¢„æµ‹ç»“æœï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if st.session_state.prediction_results is not None:
        # ä»session stateè·å–æ•°æ®
        predictions = st.session_state.prediction_results['predictions']
        feature_df = st.session_state.prediction_results['feature_df']
        feature_df_scaled = st.session_state.prediction_results['feature_df_scaled']
        params = st.session_state.prediction_params

        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        st.markdown("### Prediction Results")

        # æ˜¾ç¤ºå½“å‰é¢„æµ‹å‚æ•°
        st.info(f"ğŸ“Š Current Prediction: Street {params['selected_street']} | "
                f"ğŸ“… {params['prediction_days']} days from {params['start_date']} | "
                f"â° {params['time_granularity']}")

        # åˆ›å»ºæ—¶é—´åºåˆ—å›¾
        fig = create_prediction_timeline(predictions, params['selected_street'], params['time_granularity'])
        st.plotly_chart(fig, use_container_width=True)

        # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        col2_1, col2_2, col2_3 = st.columns(3)
        with col2_1:
            st.metric("Average Predicted Speed", f"{predictions['speed'].mean():.1f} km/h")
        with col2_2:
            st.metric("Maximum Speed", f"{predictions['speed'].max():.1f} km/h")
        with col2_3:
            st.metric("Minimum Speed", f"{predictions['speed'].min():.1f} km/h")

        # æ˜¾ç¤ºè¯¦ç»†æ•°æ®è¡¨
        with st.expander("View Detailed Prediction Data"):
            st.dataframe(
                predictions[['datetime', 'speed', 'confidence']].style.format({
                    'speed': '{:.1f} km/h',
                    'confidence': '{:.1%}'
                }),
                use_container_width=True
            )

        # -------------- SHAP åˆ†æï¼ˆé’ˆå¯¹æœ¬æ¬¡é¢„æµ‹ï¼‰ --------------
        st.markdown("## ğŸ” SHAP Value Analysis for This Prediction")

        if st.session_state.shap_results is None:
            st.info(
                "SHAP value calculation failed or unavailable. Please ensure the model supports SHAP (tree models) and feature order matches training.")
        else:
            shap_vals = st.session_state.shap_results['shap_vals']

            # æ—¶é—´ç‚¹é€‰æ‹©ä¸ä¼šè§¦å‘é¡µé¢é‡æ–°è®¡ç®—
            st.markdown("### Single Point SHAP Decomposition")

            # æ ¼å¼åŒ–æ—¶é—´ç”¨äºæ˜¾ç¤º
            time_labels = [pd.to_datetime(t).strftime("%Y-%m-%d %H:%M:%S") for t in
                           predictions['datetime'].tolist()]

            if len(time_labels) > 0:
                # ä½¿ç”¨unique keyé¿å…ä¸å…¶ä»–selectboxå†²çª
                selected_idx = st.selectbox(
                    "Select Time Point (View SHAP decomposition for this time)",
                    options=list(range(len(time_labels))),
                    format_func=lambda i: time_labels[i],
                    index=0,
                    key="shap_time_selector"  # æ·»åŠ unique key
                )

                if selected_idx is not None:
                    sample_shap = np.array(shap_vals)[selected_idx]
                    sample_feature_values = feature_df.iloc[selected_idx]

                    shap_df = pd.DataFrame({
                        'feature': feature_names,
                        'feature_value': sample_feature_values.values,
                        'shap_value': sample_shap
                    })
                    shap_df['abs_shap'] = shap_df['shap_value'].abs()
                    shap_df = shap_df.sort_values('abs_shap', ascending=False)

                    # æ¨ªå‘æ¡å½¢å›¾ï¼šæŒ‰è´¡çŒ®å¤§å°æ’åºï¼Œæ­£è´Ÿç”¨é¢œè‰²åŒºåˆ†
                    fig_shap = go.Figure(go.Bar(
                        x=shap_df['shap_value'],
                        y=shap_df['feature'],
                        orientation='h',
                        marker=dict(
                            color=shap_df['shap_value'].apply(lambda v: '#4ECDC4' if v >= 0 else '#FF6B6B')
                        ),
                        text=shap_df['shap_value'].round(3),
                        textposition='outside'
                    ))
                    fig_shap.update_layout(
                        title=f"SHAP Decomposition â€” {time_labels[selected_idx]}",
                        xaxis_title="SHAP Value (Impact on prediction: positiveâ†’increases, negativeâ†’decreases)",
                        yaxis_title="Feature",
                        height=max(400, len(shap_df) * 18),
                        margin=dict(l=200)
                    )
                    st.plotly_chart(fig_shap, use_container_width=True)

                    # æ˜¾ç¤ºç‰¹å¾å€¼ä¸ SHAP å€¼è¡¨
                    with st.expander("View Feature Values and SHAP Details for This Time Point"):
                        st.dataframe(
                            shap_df[['feature', 'feature_value', 'shap_value']].reset_index(drop=True).style.format(
                                {
                                    'feature_value': '{:.4f}',
                                    'shap_value': '{:.4f}'
                                }),
                            use_container_width=True
                        )

            # æ˜¾ç¤ºæ‰€æœ‰é¢„æµ‹æ—¶é—´ç‚¹çš„å¹³å‡ |SHAP|ï¼ˆå¸®åŠ©æŠŠæ¡æœ¬æ¬¡é¢„æµ‹åŒºé—´çš„æ•´ä½“é‡è¦æ€§åˆ†å¸ƒï¼‰
            # st.markdown("### Average |SHAP| for This Prediction Period (Sorted by Feature)")
            # mean_abs_shap = np.mean(np.abs(shap_vals), axis=0)
            # mean_shap_df = pd.DataFrame({
            #     'feature': feature_names,
            #     'mean_abs_shap': mean_abs_shap
            # }).sort_values('mean_abs_shap', ascending=False)
            #
            # fig_mean = go.Figure(go.Bar(
            #     x=mean_shap_df['mean_abs_shap'].head(30)[::-1],
            #     y=mean_shap_df['feature'].head(30)[::-1],
            #     orientation='h',
            #     text=mean_shap_df['mean_abs_shap'].head(30)[::-1].round(3),
            #     textposition='outside'
            # ))
            # fig_mean.update_layout(
            #     title="Average |SHAP| for This Prediction Period",
            #     xaxis_title="Average |SHAP|",
            #     yaxis_title="Feature",
            #     height=max(400, len(mean_shap_df.head(30)) * 20),
            #     margin=dict(l=200)
            # )
            # st.plotly_chart(fig_mean, use_container_width=True)
            # è°ƒç”¨ç€‘å¸ƒå›¾å‡½æ•°
            mean_shap_signed = np.mean(shap_vals, axis=0)
            avg_prediction = predictions['speed'].mean()
            try:
                explainer = st.session_state.shap_results['explainer']
                base_value = explainer.expected_value
                if isinstance(base_value, np.ndarray):
                    base_value = float(base_value[0])
                else:
                    base_value = float(base_value)
            except:
                # å¦‚æœæ— æ³•è·å–åŸºå‡†å€¼ï¼Œä½¿ç”¨è®­ç»ƒæ•°æ®çš„å¹³å‡å€¼ä½œä¸ºä¼°è®¡
                base_value = avg_prediction - np.sum(mean_shap_signed)
            waterfall_fig = create_shap_waterfall_plot(
                feature_names, mean_shap_signed, base_value, avg_prediction
            )
            st.plotly_chart(waterfall_fig, use_container_width=True)

    else:
        # å¦‚æœæ²¡æœ‰é¢„æµ‹ç»“æœï¼Œæ˜¾ç¤ºæç¤º
        st.info(
            "ğŸ‘ˆ Configure prediction settings in the sidebar and click 'Start Prediction' to generate predictions and SHAP analysis")


def generate_future_predictions(model, scaler, feature_names, original_df,
                                selected_street, start_date, prediction_days, time_granularity):
    """ç”Ÿæˆæœªæ¥é¢„æµ‹æ•°æ®å¹¶è¿”å›ï¼špredictions_df, feature_df, feature_df_scaled"""
    predictions = []

    # ç”Ÿæˆæ—¶é—´ç‚¹
    if time_granularity == "Hourly":
        time_points = []
        for day in range(prediction_days):
            current_date = start_date + timedelta(days=day)
            for hour in range(24):
                time_points.append(datetime.combine(current_date, datetime.min.time()) + timedelta(hours=hour))
    elif time_granularity == "Peak Hours":
        time_points = []
        for day in range(prediction_days):
            current_date = start_date + timedelta(days=day)
            # æ—©é«˜å³° 7-9ç‚¹, æ™šé«˜å³° 17-19ç‚¹
            for hour in [7, 8, 9, 17, 18, 19]:
                time_points.append(datetime.combine(current_date, datetime.min.time()) + timedelta(hours=hour))
    else:  # Daily Average
        time_points = []
        for day in range(prediction_days):
            current_date = start_date + timedelta(days=day)
            time_points.append(datetime.combine(current_date, datetime.min.time()) + timedelta(hours=12))

    # æ„å»ºæ‰€æœ‰æ—¶é—´ç‚¹çš„ç‰¹å¾çŸ©é˜µï¼ˆåŸå§‹ï¼‰
    feature_matrix = []
    for time_point in time_points:
        fv = build_feature_vector(
            original_df, selected_street, time_point, feature_names
        )
        feature_matrix.append(fv)

    # è½¬ä¸º DataFrameï¼ˆåŸå§‹ç‰¹å¾å€¼ï¼‰
    if len(feature_matrix) == 0:
        feature_df = pd.DataFrame(columns=feature_names)
    else:
        feature_df = pd.DataFrame(feature_matrix, columns=feature_names)

    # æ ‡å‡†åŒ–ï¼ˆå°½é‡ä¸€æ¬¡æ€§è½¬æ¢ï¼‰
    if scaler is not None and len(feature_df) > 0:
        try:
            feature_matrix_scaled = scaler.transform(feature_df.values)
        except Exception as e:
            # å¦‚æœç¼©æ”¾å¤±è´¥ï¼Œé€€å›åŸå§‹å€¼å¹¶è®°å½•è­¦å‘Šï¼ˆä¸Šå±‚ä¼šæ•è·ï¼‰
            feature_matrix_scaled = feature_df.values
    else:
        feature_matrix_scaled = feature_df.values

    # é¢„æµ‹ï¼ˆä¸€æ¬¡æ€§ï¼‰
    try:
        preds = model.predict(feature_matrix_scaled)
    except Exception as e:
        # å›é€€åˆ°é€ä¸ªé¢„æµ‹ï¼ˆæ›´é²æ£’ï¼Œä½†æ…¢ï¼‰
        preds = []
        for row in feature_matrix_scaled:
            try:
                preds.append(model.predict([row])[0])
            except:
                preds.append(np.nan)
        preds = np.array(preds)

    # ä¸ºæ¯ä¸ªæ—¶é—´ç‚¹æ„å»ºé¢„æµ‹è®°å½•
    for i, time_point in enumerate(time_points):
        pred_speed = float(preds[i]) if i < len(preds) else np.nan

        # è®¡ç®—ç½®ä¿¡åº¦ï¼ˆä½¿ç”¨ä¹‹å‰çš„å‡½æ•°ï¼‰
        confidence = calculate_confidence(pred_speed, time_point)

        predictions.append({
            'datetime': time_point,
            'speed': pred_speed,
            'confidence': confidence,
            'street': selected_street
        })

    predictions_df = pd.DataFrame(predictions)
    # ä¿è¯ datetime åˆ—ä¸º pd.Timestamp
    if not predictions_df.empty:
        predictions_df['datetime'] = pd.to_datetime(predictions_df['datetime'])

    # feature_df_scaled ä¹Ÿè½¬æ¢ä¸º DataFrame æ–¹ä¾¿åç»­ç´¢å¼•ï¼ˆåˆ—åä¸ feature_names å¯¹é½ï¼‰
    feature_df_scaled = pd.DataFrame(feature_matrix_scaled, columns=feature_names) if len(
        feature_matrix_scaled) > 0 else pd.DataFrame(columns=feature_names)

    return predictions_df, feature_df, feature_df_scaled


def build_feature_vector(original_df, selected_street, time_point, feature_names):
    """æ„å»ºç‰¹å¾å‘é‡ï¼ˆæ ¹æ® feature_names é¡ºåºï¼‰"""
    feature_vector = []

    # å°è¯•è¯†åˆ«è¡—é“åˆ—
    street_col = find_street_column(original_df)

    for feature in feature_names:
        if 'hour' in feature.lower():
            feature_vector.append(time_point.hour)
        elif 'weekend' in feature.lower():
            feature_vector.append(1 if time_point.weekday() >= 5 else 0)
        elif 'peak' in feature.lower():
            is_peak = time_point.hour in [7, 8, 9, 17, 18, 19]
            feature_vector.append(1 if is_peak else 0)
        elif original_df is not None and feature in original_df.columns:
            # ä»å†å²æ•°æ®ä¸­è·å–è¯¥è¡—é“çš„å¹³å‡å€¼ï¼ˆæŒ‰è¯†åˆ«åˆ°çš„è¡—é“åˆ—ç­›é€‰ï¼‰
            if street_col is not None:
                street_data = original_df[original_df[street_col] == selected_street]
            else:
                # å¦‚æœæ²¡æœ‰è¡—é“åˆ—ï¼Œåˆ™å°è¯•ä½¿ç”¨ index æˆ–ç©ºè¡¨
                if 'index' in original_df.columns:
                    street_data = original_df[original_df['index'] == selected_street]
                else:
                    street_data = pd.DataFrame()

            if not street_data.empty and feature in street_data.columns:
                # ä½¿ç”¨è¯¥è¡—é“å†å²ä¸Šçš„å‡å€¼ä½œä¸ºé»˜è®¤
                feature_vector.append(street_data[feature].mean())
            else:
                # å½“æ— å†å²æ—¶ç”¨ 0 å¡«å……ï¼ˆå¯æ ¹æ®éœ€æ±‚æ›¿æ¢ä¸ºå…¶ä»–ç­–ç•¥ï¼‰
                feature_vector.append(0.0)
        else:
            # é»˜è®¤å ä½
            feature_vector.append(0.0)

    return feature_vector


def calculate_confidence(pred_speed, time_point):
    """è®¡ç®—é¢„æµ‹ç½®ä¿¡åº¦"""
    # åŸºäºæ—¶é—´å’Œé€Ÿåº¦çš„ç®€å•ç½®ä¿¡åº¦ä¼°ç®—
    base_confidence = 0.85

    # å·¥ä½œæ—¥çš„ç½®ä¿¡åº¦æ›´é«˜
    if time_point.weekday() < 5:
        base_confidence += 0.05

    # é«˜å³°æ—¶æ®µçš„ç½®ä¿¡åº¦è¾ƒä½ï¼ˆæ›´å¤šä¸ç¡®å®šæ€§ï¼‰
    if time_point.hour in [7, 8, 9, 17, 18, 19]:
        base_confidence -= 0.1

    # é€Ÿåº¦åœ¨æ­£å¸¸èŒƒå›´å†…çš„ç½®ä¿¡åº¦æ›´é«˜
    if 30 <= pred_speed <= 80:
        base_confidence += 0.05

    return max(0.6, min(0.95, base_confidence))


def create_prediction_timeline(predictions, selected_street, time_granularity):
    """åˆ›å»ºé¢„æµ‹æ—¶é—´çº¿å›¾"""
    fig = go.Figure()

    # æ·»åŠ é¢„æµ‹çº¿
    fig.add_trace(go.Scatter(
        x=predictions['datetime'],
        y=predictions['speed'],
        mode='lines+markers',
        name='Predicted Speed',
        line=dict(color='#2E86AB', width=2),
        marker=dict(size=8, color='#2E86AB'),
        hovertemplate='Time: %{x}<br>Speed: %{y:.1f} km/h<extra></extra>'
    ))

    # æ·»åŠ ç½®ä¿¡åŒºé—´
    upper_bound = predictions['speed'] * (1 + (1 - predictions['confidence']))
    lower_bound = predictions['speed'] * (1 - (1 - predictions['confidence']))

    fig.add_trace(go.Scatter(
        x=predictions['datetime'].tolist() + predictions['datetime'].tolist()[::-1],
        y=upper_bound.tolist() + lower_bound.tolist()[::-1],
        fill='toself',
        fillcolor='rgba(46, 134, 171, 0.2)',
        line=dict(color='rgba(255,255,255,0)'),
        showlegend=False,
        hoverinfo='skip'
    ))

    # æ ‡è®°é«˜å³°æ—¶æ®µ
    if time_granularity == "Hourly":
        for idx, row in predictions.iterrows():
            if row['datetime'].hour in [7, 8, 9, 17, 18, 19]:
                fig.add_vline(
                    x=row['datetime'],
                    line_dash="dot",
                    line_color="orange",
                    opacity=0.3
                )

    fig.update_layout(
        title=f"Street {selected_street} - Speed Prediction for Next {len(predictions)} Time Points",
        xaxis_title="Time",
        yaxis_title="Predicted Speed (km/h)",
        height=400,
        hovermode='x unified',
        showlegend=True,
        legend=dict(
            yanchor="top",
            y=0.99,
            xanchor="left",
            x=0.01
        )
    )

    # æ·»åŠ ç½‘æ ¼
    fig.update_xaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')
    fig.update_yaxes(showgrid=True, gridwidth=1, gridcolor='LightGray')

    return fig


def create_shap_waterfall_plot(feature_names, mean_shap_values, base_value, final_prediction, top_n=15):
    """
    åˆ›å»ºSHAPç€‘å¸ƒå›¾ï¼Œæ˜¾ç¤ºç‰¹å¾å¦‚ä½•ä»åŸºå‡†å€¼é€æ­¥è´¡çŒ®åˆ°æœ€ç»ˆé¢„æµ‹å€¼

    Parameters:
    - feature_names: ç‰¹å¾åç§°åˆ—è¡¨
    - mean_shap_values: å¹³å‡SHAPå€¼ï¼ˆä¿ç•™æ­£è´Ÿï¼‰
    - base_value: æ¨¡å‹åŸºå‡†å€¼
    - final_prediction: æœ€ç»ˆé¢„æµ‹å€¼
    - top_n: æ˜¾ç¤ºå‰Nä¸ªæœ€é‡è¦çš„ç‰¹å¾
    """

    # åˆ›å»ºç‰¹å¾è´¡çŒ®DataFrameå¹¶æŒ‰ç»å¯¹å€¼æ’åº
    contrib_df = pd.DataFrame({
        'feature': feature_names,
        'contribution': mean_shap_values,
        'abs_contribution': np.abs(mean_shap_values)
    }).sort_values('abs_contribution', ascending=False)

    # åªå–å‰Nä¸ªæœ€é‡è¦çš„ç‰¹å¾ï¼Œå…¶ä½™åˆå¹¶ä¸º"Other Features"
    if len(contrib_df) > top_n:
        top_features = contrib_df.head(top_n)
        other_contribution = contrib_df.tail(len(contrib_df) - top_n)['contribution'].sum()

        # æ·»åŠ "Other Features"é¡¹
        other_row = pd.DataFrame({
            'feature': ['Other Features'],
            'contribution': [other_contribution],
            'abs_contribution': [abs(other_contribution)]
        })
        waterfall_data = pd.concat([top_features, other_row], ignore_index=True)
    else:
        waterfall_data = contrib_df

    # è®¡ç®—ç€‘å¸ƒå›¾çš„ç´¯ç§¯å€¼
    cumulative_values = [base_value]
    for contrib in waterfall_data['contribution']:
        cumulative_values.append(cumulative_values[-1] + contrib)

    # åˆ›å»ºç€‘å¸ƒå›¾
    fig = go.Figure()

    # æ·»åŠ åŸºå‡†å€¼
    fig.add_trace(go.Bar(
        name='Baseline',
        x=['Baseline'],
        y=[base_value],
        marker_color='lightgray',
        text=[f'{base_value:.2f}'],
        textposition='outside',
        showlegend=True
    ))

    # æ·»åŠ æ¯ä¸ªç‰¹å¾çš„è´¡çŒ®
    x_labels = ['Baseline']
    colors = []
    y_values = [base_value]

    for i, (_, row) in enumerate(waterfall_data.iterrows()):
        feature = row['feature']
        contrib = row['contribution']

        # æˆªæ–­è¿‡é•¿çš„ç‰¹å¾å
        display_name = feature if len(feature) <= 20 else feature[:17] + '...'
        x_labels.append(display_name)

        # ç¡®å®šé¢œè‰²ï¼šæ­£è´¡çŒ®ä¸ºç»¿è‰²ï¼Œè´Ÿè´¡çŒ®ä¸ºçº¢è‰²
        color = '#2E8B57' if contrib > 0 else '#DC143C'  # æ·±ç»¿ vs æ·±çº¢
        colors.append(color)

        # å¯¹äºç€‘å¸ƒå›¾ï¼Œéœ€è¦æ˜¾ç¤ºå¢é‡æ¡å½¢å›¾
        if contrib > 0:
            # æ­£è´¡çŒ®ï¼šä»å‰ä¸€ä¸ªå€¼å¼€å§‹å‘ä¸Š
            base = cumulative_values[i]
            height = contrib
        else:
            # è´Ÿè´¡çŒ®ï¼šä»å‰ä¸€ä¸ªå€¼+è´¡çŒ®å¼€å§‹å‘ä¸Šï¼ˆè´¡çŒ®ä¸ºè´Ÿå€¼ï¼‰
            base = cumulative_values[i + 1]
            height = -contrib  # è½¬ä¸ºæ­£å€¼ç”¨äºæ˜¾ç¤º

        fig.add_trace(go.Bar(
            name=display_name,
            x=[display_name],
            y=[height],
            base=base,
            marker_color=color,
            text=[f'{contrib:+.2f}'],
            textposition='outside',
            showlegend=False,
            hovertemplate=f'<b>{feature}</b><br>Contribution: {contrib:+.3f}<br>Cumulative: {cumulative_values[i + 1]:.3f}<extra></extra>'
        ))

    # æ·»åŠ æœ€ç»ˆé¢„æµ‹å€¼
    fig.add_trace(go.Bar(
        name='Final Prediction',
        x=['Final Prediction'],
        y=[final_prediction],
        marker_color='#2E86AB',
        text=[f'{final_prediction:.2f}'],
        textposition='outside',
        showlegend=True
    ))

    # æ·»åŠ è¿æ¥çº¿æ˜¾ç¤ºç´¯ç§¯æ•ˆæœ
    x_positions = list(range(len(x_labels) + 1))
    fig.add_trace(go.Scatter(
        x=x_positions,
        y=cumulative_values,
        mode='lines+markers',
        line=dict(color='black', width=1, dash='dot'),
        marker=dict(color='black', size=4),
        name='Cumulative Value',
        showlegend=True,
        hovertemplate='Cumulative: %{y:.3f}<extra></extra>'
    ))

    # æ›´æ–°å¸ƒå±€
    fig.update_layout(
        title=f"SHAP Waterfall Plot - How Features Build Up the Prediction",
        xaxis_title="Features (ordered by importance)",
        yaxis_title="Predicted Speed (km/h)",
        height=500,
        barmode='overlay',
        hovermode='x unified',
        legend=dict(
            orientation="h",
            yanchor="bottom",
            y=1.02,
            xanchor="right",
            x=1
        ),
        annotations=[
            dict(
                x=0,
                y=base_value + max(abs(base_value) * 0.1, 2),
                text=f"Baseline: {base_value:.2f}",
                showarrow=True,
                arrowhead=2,
                arrowcolor="gray",
                font=dict(color="gray", size=10)
            ),
            dict(
                x=len(x_labels),
                y=final_prediction + max(abs(final_prediction) * 0.1, 2),
                text=f"Final: {final_prediction:.2f}",
                showarrow=True,
                arrowhead=2,
                arrowcolor="#2E86AB",
                font=dict(color="#2E86AB", size=10)
            )
        ]
    )

    # è°ƒæ•´xè½´æ ‡ç­¾è§’åº¦ä»¥é€‚åº”é•¿ç‰¹å¾å
    fig.update_xaxes(tickangle=45)

    return fig


def create_feature_importance_plot(model, feature_names, show_all=True):
    """åˆ›å»ºç‰¹å¾é‡è¦æ€§å›¾ï¼ˆæ˜¾ç¤ºå…¨éƒ¨ç‰¹å¾ï¼‰"""
    # è·å–ç‰¹å¾é‡è¦æ€§
    importances = model.feature_importances_

    # åˆ›å»ºDataFrame
    importance_df = pd.DataFrame({
        'feature': feature_names,
        'importance': importances
    }).sort_values('importance', ascending=False)

    # å¦‚æœä¸æ˜¾ç¤ºå…¨éƒ¨ï¼Œåªæ˜¾ç¤ºå‰Nä¸ª
    if not show_all and len(importance_df) > 15:
        importance_df = importance_df.head(15)

    # åˆ›å»ºæ¡å½¢å›¾
    fig = go.Figure(go.Bar(
        x=importance_df['importance'],
        y=importance_df['feature'],
        orientation='h',
        marker=dict(
            color=importance_df['importance'],
            colorscale='Viridis',
            showscale=True,
            colorbar=dict(title="Importance")
        ),
        text=importance_df['importance'].round(4),
        textposition='outside'
    ))

    fig.update_layout(
        title="Feature Importance Analysis (All Features)" if show_all else "Feature Importance Analysis (Top 15)",
        xaxis_title="Importance Score",
        yaxis_title="Feature",
        height=max(400, len(importance_df) * 25),  # æ ¹æ®ç‰¹å¾æ•°é‡è°ƒæ•´é«˜åº¦
        yaxis={'categoryorder': 'total ascending'},
        margin=dict(l=150)  # å¢åŠ å·¦è¾¹è·ä»¥å®¹çº³ç‰¹å¾åç§°
    )

    return fig


def create_shap_summary_with_all_features(shap_values, X_sample):
    """åˆ›å»ºæ˜¾ç¤ºæ‰€æœ‰ç‰¹å¾çš„SHAPæ‘˜è¦å›¾"""
    fig = go.Figure()

    # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„å¹³å‡ç»å¯¹SHAPå€¼
    mean_abs_shap = np.abs(shap_values).mean(axis=0)
    feature_importance = pd.DataFrame({
        'feature': X_sample.columns,
        'importance': mean_abs_shap
    }).sort_values('importance', ascending=False)

    # æ˜¾ç¤ºæ‰€æœ‰ç‰¹å¾
    for idx, feature in enumerate(feature_importance['feature']):
        feature_idx = X_sample.columns.get_loc(feature)

        # è·å–è¯¥ç‰¹å¾çš„SHAPå€¼å’Œç‰¹å¾å€¼
        shap_vals = shap_values[:, feature_idx]
        feature_vals = X_sample[feature].values

        # å½’ä¸€åŒ–ç‰¹å¾å€¼ç”¨äºé¢œè‰²æ˜ å°„
        if feature_vals.std() > 0:
            feature_vals_norm = (feature_vals - feature_vals.min()) / (feature_vals.max() - feature_vals.min())
        else:
            feature_vals_norm = np.zeros_like(feature_vals)

        # æ·»åŠ æ•£ç‚¹ï¼ˆä½¿ç”¨è¾ƒå°çš„ç‚¹ä»¥å®¹çº³æ›´å¤šç‰¹å¾ï¼‰
        fig.add_trace(go.Scatter(
            x=shap_vals,
            y=[idx] * len(shap_vals) + np.random.normal(0, 0.05, len(shap_vals)),
            mode='markers',
            marker=dict(
                size=3,
                color=feature_vals_norm,
                colorscale='RdBu_r',
                showscale=(idx == 0),
                colorbar=dict(
                    title="Feature Value<br>(Normalized)",
                    x=1.02
                ),
                opacity=0.6
            ),
            name=feature,
            hovertemplate=f'{feature}<br>SHAP Value: %{{x:.3f}}<br>Feature Value: %{{marker.color:.3f}}<extra></extra>'
        ))

    fig.update_layout(
        title=f"SHAP Value Summary Plot (All {len(feature_importance)} Features)",
        xaxis_title="SHAP Value (Impact on Model Output)",
        yaxis=dict(
            tickmode='array',
            tickvals=list(range(len(feature_importance))),
            ticktext=feature_importance['feature'].tolist()[::-1],
            title=""
        ),
        height=max(500, len(feature_importance) * 20),  # æ ¹æ®ç‰¹å¾æ•°é‡è°ƒæ•´é«˜åº¦
        showlegend=False,
        hovermode='closest',
        margin=dict(l=150)  # å¢åŠ å·¦è¾¹è·
    )

    return fig


# è®¡ç®—ç‰¹å¾äº¤äº’
def calculate_feature_interactions(shap_values, X_sample, feature_names, n_top_interactions=10):
    """
    è®¡ç®—ç‰¹å¾é—´çš„äº¤äº’æ•ˆåº”

    Parameters:
    - shap_values: SHAP values array
    - X_sample: sample data
    - feature_names: list of feature names
    - n_top_interactions: number of top interactions to return

    Returns:
    - DataFrame with interaction scores
    """
    interaction_matrix = np.zeros((len(feature_names), len(feature_names)))

    for i in range(len(feature_names)):
        for j in range(i + 1, len(feature_names)):
            try:
                # è®¡ç®—ç‰¹å¾å€¼ä¹‹é—´çš„ç›¸å…³æ€§
                feature_corr = np.corrcoef(X_sample.iloc[:, i], X_sample.iloc[:, j])[0, 1]

                # è®¡ç®—SHAPå€¼ä¹‹é—´çš„ç›¸å…³æ€§
                shap_corr = np.corrcoef(shap_values[:, i], shap_values[:, j])[0, 1]

                # äº¤äº’å¼ºåº¦ = |ç‰¹å¾ç›¸å…³æ€§ * SHAPç›¸å…³æ€§|
                interaction_score = np.abs(feature_corr * shap_corr)

                interaction_matrix[i, j] = interaction_score
                interaction_matrix[j, i] = interaction_score

            except:
                interaction_matrix[i, j] = 0
                interaction_matrix[j, i] = 0

    # æå–ä¸Šä¸‰è§’çŸ©é˜µçš„äº¤äº’å¯¹
    interactions = []
    for i in range(len(feature_names)):
        for j in range(i + 1, len(feature_names)):
            interactions.append({
                'Feature1': feature_names[i],
                'Feature2': feature_names[j],
                'Interaction_Score': interaction_matrix[i, j]
            })

    # æŒ‰äº¤äº’å¼ºåº¦æ’åº
    interactions_df = pd.DataFrame(interactions).sort_values(
        'Interaction_Score', ascending=False
    ).head(n_top_interactions)

    return interactions_df, interaction_matrix


def main():
    # æ ‡é¢˜
    st.markdown('<h1 class="main-header">ğŸš— Traffic Speed Prediction Analysis Platform</h1>',
                unsafe_allow_html=True)

    # åŠ è½½æ¨¡å‹å’Œæ•°æ®
    with st.spinner('Loading the model and data...'):
        models_dict, preprocessing_info, results_df = load_models()
        original_df = load_original_data()

    if models_dict is None or preprocessing_info is None:
        st.error(
            "The necessary model files cannot be loaded. Please ensure that all files are in the correct location.")
        return

    # ä½¿ç”¨æœ€ä½³æ¨¡å‹
    best_model = models_dict['best_model']
    X_test = preprocessing_info['X_test']
    y_test = preprocessing_info['y_test']
    feature_names = preprocessing_info['feature_names']
    scaler = preprocessing_info['scaler']

    # ä¾§è¾¹æ é…ç½®
    with st.sidebar:
        st.title("âš™ï¸ Model Information")

        # æ˜¾ç¤ºæ¨¡å‹åŸºæœ¬ä¿¡æ¯
        st.subheader("ğŸ“Š Model Performance")
        try:
            y_pred = best_model.predict(X_test)
            current_r2 = r2_score(y_test, y_pred)
            current_rmse = np.sqrt(mean_squared_error(y_test, y_pred))
            current_mae = mean_absolute_error(y_test, y_pred)

            # st.metric("æ¨¡å‹ç±»å‹", "XGBoost æœ€ä½³æ¨¡å‹")
            col1, col2 = st.columns(2)
            with col1:
                st.metric("RÂ² Score", f"{current_r2:.4f}")
                st.metric("RMSE", f"{current_rmse:.4f}")
            with col2:
                st.metric("MAE", f"{current_mae:.4f}")
                # st.metric("æµ‹è¯•æ ·æœ¬æ•°", f"{len(X_test)}")
        except Exception as e:
            st.error(f"æ— æ³•è®¡ç®—æ¨¡å‹æ€§èƒ½: {str(e)}")

        st.divider()

        # SHAPåˆ†æé…ç½®
        # st.subheader("åˆ†æé…ç½®")
        shap_sample_size = 1000
        # shap_sample_size = st.slider(
        #     "SHAPåˆ†ææ ·æœ¬æ•°",
        #     min_value=50,
        #     max_value=500,
        #     value=100,
        #     step=50,
        #     help="æ›´å¤šæ ·æœ¬ä¼šæä¾›æ›´å‡†ç¡®çš„åˆ†æï¼Œä½†è®¡ç®—æ—¶é—´æ›´é•¿"
        # )
        show_all_features = True
        # show_all_features = st.checkbox(
        #     "æ˜¾ç¤ºæ‰€æœ‰ç‰¹å¾",
        #     value=True,
        #     help="å‹¾é€‰ä»¥æ˜¾ç¤ºæ‰€æœ‰ç‰¹å¾çš„é‡è¦æ€§"
        # )

    # ä¸»ç•Œé¢ï¼šæœªæ¥é¢„æµ‹
    st.header("ğŸ”® Prediction of Future Traffic Speeds")
    try:
        create_future_prediction_interface(best_model, scaler, feature_names, original_df)
    except Exception as e:
        st.error(f"é¢„æµ‹ç•Œé¢åˆå§‹åŒ–å¤±è´¥: {str(e)}")


if __name__ == "__main__":
    main()